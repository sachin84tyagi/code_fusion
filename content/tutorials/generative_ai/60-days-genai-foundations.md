# ðŸš€ GENAI FOUNDATIONS â€” DAYS 1â€“3

**Goal:** Build strong intuition of AI â†’ LLMs â†’ Transformers, understand tokens/embeddings/attention, set up environment, and run your first Helloâ€‘LLM (API + Local).

---

# DAY 1 â€” AI / ML / DL / GENERATIVE AI (CORE INTUITION)

## 1. What is AI?

Artificial Intelligence (AI) = Machines performing tasks that normally require human intelligence.
Examples: Chatbots, Recommendation Systems, Selfâ€‘Driving, Voice Assistants.

## 2. Machine Learning (ML)

ML = Machines learn patterns from data instead of explicit programming.

Example:
Spam Detection â†’ Learn from thousands of emails.

## 3. Deep Learning (DL)

DL = Subset of ML using Neural Networks with many layers.
Used for: Vision, Speech, Language, Generative AI.

## 4. Generative AI

Generative AI = Models that CREATE new content.

Creates:

- Text (ChatGPT)
- Images (Stable Diffusion)
- Code (Copilot)
- Audio / Video

Key Idea: Learn probability of next token.

---

## AI â†’ ML â†’ DL â†’ GENAI RELATION

```
Artificial Intelligence
    â”œâ”€â”€ Machine Learning
    â”‚     â”œâ”€â”€ Deep Learning
    â”‚     â”‚     â”œâ”€â”€ Generative AI
```

---

## Mini Lab â€” Understanding Probability

Sentence: "The sky is \_\_\_"
Model predicts:

- blue (0.85)
- dark (0.08)
- green (0.01)

Model chooses highest probability.

---

# DAY 2 â€” LLM + TRANSFORMER INTUITION

## 1. What is LLM?

LLM = Large Language Model trained on massive text.
Goal: Predict next token.

Examples:

- GPT
- LLaMA
- Mistral

---

## 2. Transformer â€” Core Idea

Transformer = Architecture that understands CONTEXT using ATTENTION.

Before Transformer â†’ RNN (slow, forget context)
After Transformer â†’ Parallel, long memory, powerful.

---

## Transformer Flow (High Level)

```
Input Text â†’ Tokens â†’ Embeddings â†’ Transformer Blocks â†’ Output Tokens
```

---

## 3. Tokens (Conceptual)

Token = Smallest unit model understands.

Example:
"Generative AI is amazing"
â†’ ["Gener", "ative", " AI", " is", " amazing"]

Model works on NUMBERS not text.

---

## 4. Embeddings

Embedding = Convert token â†’ vector (meaning in numbers).

Example:
King â‰ˆ Queen (similar vectors)
Paris â‰ˆ France

Embedding captures SEMANTIC MEANING.

---

## 5. Attention (The MAGIC)

Attention = Each word looks at other words to understand meaning.

Example:
"The animal didn't cross the road because IT was tired"
"IT" refers to â†’ animal (model learns using attention).

---

## Selfâ€‘Attention Intuition

```
Every word asks:
Which other words are important for me?
```

---

## Mini Lab â€” Human Attention

Sentence: "I deposited money in the bank"

Possible meanings of "bank":

- River bank
- Financial bank

Context decides meaning â†’ Same as attention.

---

# DAY 3 â€” TOKENS / EMBEDDINGS / SETUP / HELLOâ€‘LLM

## 1. Environment Setup

### Install Python

Download â†’ [https://python.org](https://python.org)

Verify:

```
python --version
```

### Install VS Code + Extensions

- Python
- Pylance
- Jupyter (optional)

### Install Git

```
git --version
```

---

## Optional GPU Setup

Only needed for LOCAL models / training.

- Install NVIDIA Drivers
- Install CUDA
- Install PyTorch with CUDA

Check:

```
python -c "import torch; print(torch.cuda.is_available())"
```

---

## 2. Python Virtual Environment

```
python -m venv venv
venv\\Scripts\\activate   (Windows)
source venv/bin/activate   (Mac/Linux)
```

Install basic libs:

```
pip install openai transformers torch python-dotenv
```

---

## 3. HELLOâ€‘LLM (API VERSION)

Create file: `hello_llm_api.py`

```python
from openai import OpenAI
client = OpenAI()

response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": "Explain Generative AI in one line"}]
)

print(response.choices[0].message.content)
```

Run:

```
python hello_llm_api.py
```

---

## 4. HELLOâ€‘LLM (LOCAL MODEL â€” HuggingFace)

Create file: `hello_llm_local.py`

```python
from transformers import pipeline

pipe = pipeline("text-generation", model="distilgpt2")

result = pipe("Generative AI is", max_length=30)
print(result[0]['generated_text'])
```

Run:

```
python hello_llm_local.py
```

---

## What You Learned (Days 1â€“3)

- AI vs ML vs DL vs Generative AI
- LLM intuition
- Transformer highâ€‘level flow
- Tokens / Embeddings / Attention concept
- Python + VS Code + Git setup
- Virtual environment
- API LLM call
- Local LLM run

---

## Realâ€‘World Understanding

When you use ChatGPT:

1. Text â†’ Tokens
2. Tokens â†’ Embeddings
3. Transformer processes context using Attention
4. Model predicts next token
5. Tokens â†’ Text output

You now understand the CORE of GenAI ðŸš€

---

## Next (Day 4â€“7 Preview)

- Prompt Engineering Deep Dive
- Temperature / Topâ€‘p / Tokens control
- Chat vs Completion APIs
- Cost optimization
- Streaming responses
- Build Smart CLI Chatbot

---

**End of Days 1â€“3 â€” GenAI Foundations**

---

# ðŸš€ DAYS 4â€“7 â€” PROMPT ENGINEERING + PARAMETERS + CHATBOT

**Goal:** Master prompting, understand LLM parameters, control output, and build a smart CLI chatbot.

---

# DAY 4 â€” PROMPT ENGINEERING FUNDAMENTALS

## What is Prompt Engineering?

Prompt Engineering = Designing clear, structured inputs to get best output from LLM.

Bad Prompt:

```
Explain AI
```

Good Prompt:

```
Explain Artificial Intelligence in simple language with 3 real-world examples in 5 bullet points.
```

---

## Core Prompt Patterns

### 1. Instruction Prompt

Tell model what to do.

```
Summarize this article in 5 bullet points.
```

### 2. Role Prompt

Assign role to model.

```
Act as a senior Python engineer and explain decorators.
```

### 3. Fewâ€‘Shot Prompt

Give examples.

```
Input: 2+2
Output: 4
Input: 5+3
Output: 8
Input: 10+6
Output:
```

### 4. Chainâ€‘ofâ€‘Thought

Force reasoning.

```
Solve stepâ€‘byâ€‘step.
```

---

## Mini Lab

Improve this prompt:

```
Tell about ML
```

â†’ Add: role + structure + depth + examples.

---

# DAY 5 â€” LLM PARAMETERS (CONTROL THE MODEL)

## 1. Temperature (Creativity)

- 0 â†’ Deterministic / factual
- 0.7 â†’ Balanced
- 1.0 â†’ Creative / random

Example:

```
Temperature 0 â†’ "Sky is blue"
Temperature 1 â†’ "Sky glows with endless blue"
```

---

## 2. Topâ€‘P (Nucleus Sampling)

Controls probability mass.

- 0.9 â†’ Safer
- 1.0 â†’ Open

---

## 3. Max Tokens

Limits output length.

---

## 4. Frequency Penalty

Reduces repetition.

---

## 5. Presence Penalty

Encourages new topics.

---

## Mini Lab â€” Experiment

Change temperature = 0 vs 1 and compare output.

---

# DAY 6 â€” BUILD SMART CLI CHATBOT (API)

## Create: `chatbot.py`

```python
from openai import OpenAI
client = OpenAI()

print("AI Chatbot â€” type 'exit' to quit
")

while True:
    user = input("You: ")
    if user.lower() == "exit":
        break

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": user}],
        temperature=0.7,
        max_tokens=200
    )

    print("AI:", response.choices[0].message.content)
```

Run:

```
python chatbot.py
```

---

## Add Memory (Conversation Context)

```python
messages = []

while True:
    user = input("You: ")
    messages.append({"role": "user", "content": user})

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=messages
    )

    reply = response.choices[0].message.content
    messages.append({"role": "assistant", "content": reply})

    print("AI:", reply)
```

---

# DAY 7 â€” LOCAL CHATBOT + PROMPT MASTERY

## Local Chatbot (Transformers)

```python
from transformers import pipeline

chat = pipeline("text-generation", model="distilgpt2")

while True:
    user = input("You: ")
    result = chat(user, max_length=100)
    print("AI:", result[0]['generated_text'])
```

---

## Prompt Mastery Checklist

- Clear instruction
- Assign role
- Define format
- Limit length
- Ask reasoning
- Provide examples

---
